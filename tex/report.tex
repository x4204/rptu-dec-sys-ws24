\input{preamble.tex}

\begin{document}
\input{header.tex}

\section{Basic ideas of IPFS}

% https://docs.ipfs.tech/concepts/file-systems/#unix-file-system-unixfs
% ipfs add --help
% ipfs cat --help
\subsection{File upload, download and storage}

A file can be uploaded to IPFS by issuing \verb|ipfs add PATH...| on the
command line. The \verb|ipfs add| command is handled in
\verb|kubo/core/commands/add.go|, which later delegates everything to
\verb|UnixfsAPI.Add()|. There the file is split into smaller parts called
chunks, if necessary, and a CID is computed for every chunk. If the file is
small enough, only one chunk will be created. Additionally, layouting is
performed: file chunks are arranged into a Merkle DAG and uploaded. Metadata
about the chunks is stored in a UnixFS node. There are two strategies for
building the Merkle DAG: \verb|balanced| (default) and \verb|trickle|. When the
upload for all chunks is done, the CID of the root of the DAG is returned to
the user as a reference to the file.

A file can be downloaded from IPFS by issuing \verb|ipfs cat IPFS-PATH...| on
the command line. The CID is used to lookup the content in the DHT and find
IPFS instances that store the content linked by the CID. When downloading a
file, the Merkle DAG is traversed in in-order and the content of every chunk is
sent back. It is also possible to read only a particular byte range from the
file by specifying the offset and length.


\subsection{Peer connections}

Kubo has a hardcoded list of bootstrap nodes
(\verb|go-libp2p-kad-dht/dht_bootstrap.go| and
\verb|config/bootstrap_peers.go|). When launched, it will try to automatically
connect to them and fill its routing table with peers. From time to time (10
minutes by default), it will evict peers if not reachable
(\verb|go-libp2p-kad-dht/rtrefresh/rt_refresh_manager.go|). If an IPFS instance
is encountered while querying (\verb|go-libp2p-kad-dht/query.go| -
\verb|IpfsDHT.runQuery()|), Kubo will check if it is eligible to be added to
the routing table and add it (\verb|go-libp2p-kad-dht/dht.go| -
\verb|IpfsDHT.rtPeerLoop()|).


\section{Implementation}

The code is organized in two repositories:
\begin{itemize}
\item https://github.com/x4204/rptu-dec-sys-ws24-kubo - this repository
contains the modified source code of Kubo. All code changes can be found on
branch \verb|dec-sys|
\item https://github.com/x4204/rptu-dec-sys-ws24 - this repository contains
everything else related to the lab itself: deployment, benchmark and metric
gathering scripts as well as documentation on how to setup and run them
\end{itemize}

\textbf{Code changes.}\\
The following modifications were made to the source code of Kubo:
\begin{itemize}
\item remove hardcoded lists of bootstrap addresses in
\verb|config/bootstrap_peers.go| and \verb|go-libp2p-kad-dht/dht_bootstrap.go|.
Done because we don't want Kubo to automatically connect to them
\item disable MDNS in \verb|config/init.go|. Done, because it is hardcoded to
\verb|true| and ignores the config option \verb|Discovery.MDNS.Enabled|
\item disable \verb|dht.rtRefreshManager|, \verb|dht.rtPeerLoop()| and
\verb|dht.runFixLowPeersLoop()| in \verb|go-libp2p-kad-dht/dht.go|. These
places are responsible for triggering peer lookup and adding peers to the
routing table if any new ones were encountered while running queries
\end{itemize}

\textbf{Config changes.}\\
Docker compose is used to run an IPFS swarm. The following swarm key is mapped
into every container:
\begin{lstlisting}
/key/swarm/psk/1.0.0/
/base16/
3ffe35d22f310d2505b21a1b104f23d9e19534ae0a610d8717eada1d79e4f9b8
\end{lstlisting}

Additionally, the following commands run before the container starts:
\begin{lstlisting}
ipfs config --json Bootstrap '[]'
ipfs config --json Routing.Type '"dht"'
ipfs config --json Discovery.MDNS.Enabled 'false'
\end{lstlisting}

The first command removes all bootstrap nodes from the config file. The second
command configures Kubo to use \verb|dht| routing, which is required by Kubo
when running in swarm mode. Finally, with the third command we explicitly
disable mDNS for peer lookup in local network.

As an additional measure, we also set the environment variable
\verb|LIBP2P_FORCE_PNET=1| to force the usage of private networks, as suggested
in the documentation.

\textbf{Peer behaviour simulation.}\\
The deployment of any specific topology is automated
\footnote{https://github.com/x4204/rptu-dec-sys-ws24/blob/master/deploy/main.py}
and is capable of deploying nodes and linking them according to a given config
file as well visualising it. Topology files are in TOML configuration format
and are just a list of nodes and links between them, for example:
\begin{lstlisting}
nodes = [
  'ipfs-00',
  'ipfs-01',
  'ipfs-02',
]

links = [
  ['ipfs-00', 'ipfs-01'],
  ['ipfs-01', 'ipfs-02'],
]
\end{lstlisting}

The deployment script then generates a \verb|docker-compose.yml| file with the
specified nodes and wipes the storage. After that, it configures the nodes,
starts and waits for them all to become available and finally links them
according to the specified topology.

\textbf{Benchmarking methodology}\\
In every test the entire topology is recreated and the storage is wiped to
ensure a clean state. Container metrics are collected using
\verb|docker stats|. File sizes take values from \verb|1 MiB| to \verb|500 MiB|
and follow the zipf distribution with parameter $a=1.5$. The content of the
files is randomly generated using \verb|/dev/urandom| device as a source for
randomness. The benchmark runs for 5 minutes with 100 concurrent clients and a
85\% read/15\% write workload. Nodes are chosen uniformly at random for
upload/download.

Before the benchmark starts, and just so we have some files in the system, 4
files are uploaded to 4 different nodes, after which they are downloaded from
one neighbour and one neighbour of the neighbour so that it's distributed
across the network. Then the 100 concurrent clients start and try
uploading/downloading to/from different nodes. We set a download timeout of 30
seconds, so that if the file doesn't become available in this time interval,
then the query is dropped and the client tries to download another random file.


\newpage
\section{Results}

\subsection{Case 1: Kubo without source code modifications}

\begin{wrapfigure}{r}{0.5\textwidth}
\centering
\captionsetup{justification=centering,width=0.6\linewidth}
\includegraphics[width=0.8\linewidth]{figures/topologies/original.png}
\caption{Original Kubo -- topology}
\label{fig:original-topology}
\end{wrapfigure}

Since nodes decide on their own how to connect to each other, the
information about the topology was gathered after the benchmark. The nodes
formed a complete graph.

In case of the original Kubo implementation we notice a very smooth network
usage and disk usage. There are no nodes that have little to no activity. CPU
usage is consistent across nodes and is kept at around 50\% start to finish.

Figure \ref{fig:original-topology} displays how nodes are connected to each
other. Exact numbers for every metric are displayed in tables
\ref{tab:original-avg} and \ref{tab:original-total}. Table
\ref{tab:original-avg} contains metrics averages for every node. Similarly,
table \ref{tab:original-total} contains metrics totals for every node.

\textcolor{white}{.}\\\\\\\\\\\\

\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/original/net_read.png}
\caption{Original Kubo -- amount of bytes received over network}
\label{fig:original-net_read}
\end{figure}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/original/net_write.png}
\caption{Original Kubo -- amount of bytes sent over network}
\label{fig:original-net_write}
\end{figure}
\end{minipage}

\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/original/blk_read.png}
\caption{Original Kubo -- amount of bytes read from disk}
\label{fig:original-blk_read}
\end{figure}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/original/blk_write.png}
\caption{Original Kubo -- amount of bytes written to disk}
\label{fig:original-blk_write}
\end{figure}
\end{minipage}

\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/original/cpu_usage.png}
\caption{Original Kubo -- CPU usage}
\label{fig:original-cpu_usage}
\end{figure}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/original/mem_usage.png}
\caption{Original Kubo -- memory usage}
\label{fig:original-mem_usage}
\end{figure}
\end{minipage}

\input{tex/figures/original/avg}
\input{tex/figures/original/total}


\newpage
\subsection{Case 2: Ring Topology}

\begin{wrapfigure}{r}{0.5\textwidth}
\centering
\captionsetup{justification=centering,width=0.6\linewidth}
\includegraphics[width=0.8\linewidth]{figures/topologies/ring.png}
\caption{Ring -- topology}
\label{fig:ring-topology}

\end{wrapfigure}
It can be observed in figure \ref{fig:ring-blk_read} that disk reads don't
happen until much later in the benchmarking process. We think that is because
of the way files propagate through the network. Since a node is unlikely to be
able to talk directly to the one that has the file, it has to wait until
eventually one of its neighbours has it. This way, many initial requests for
the file just time out and we see a big interval of disk read inactivity in the
beginning.

On top of that, all metrics exhibit very uneven behaviour between nodes. There
are some nodes that eventually serve a lot of files and others that have very
little activity. The ones that are the most active are in a continuous range in
the ring, namely the nodes \verb|07| through \verb|00|. It might just happen
that by chance more files were uploaded and downloaded in the beginning
specifically in that part of the ring, which would make the file spread sooner
there.

Finally, the CPU usage is very spiky and also comes in bursts as can be seen in
figure \ref{fig:ring-cpu_usage}. We think that the silent periods are caused by
the fact that any given node has only two neighbours and it cannot directly
reach the node that has the file, so it has to wait for one of the neighbours
to get the file. The spikes are probably happening when the node finally gets
the file. Since there might be many clients waiting for the same file from the
same node, Kubo has to process and send it to all of them at the same time once
the file becomes available, hence the sudden increase in CPU usage, after which
it also quickly decreases.

Figure \ref{fig:ring-topology} displays how nodes are connected to each other.
Exact numbers for every metric are displayed in tables \ref{tab:ring-avg} and
\ref{tab:ring-total}. Table \ref{tab:ring-avg} contains metrics averages for
every node. Similarly, table \ref{tab:ring-total} contains metrics totals for
every node.

\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/ring/net_read.png}
\caption{Ring topology -- amount of bytes received over network}
\label{fig:ring-net_read}
\end{figure}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/ring/net_write.png}
\caption{Ring topology -- amount of bytes sent over network}
\label{fig:ring-net_write}
\end{figure}
\end{minipage}

\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/ring/blk_read.png}
\caption{Ring topology -- amount of bytes read from disk}
\label{fig:ring-blk_read}
\end{figure}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/ring/blk_write.png}
\caption{Ring topology -- amount of bytes written to disk}
\label{fig:ring-blk_write}
\end{figure}
\end{minipage}

\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/ring/cpu_usage.png}
\caption{Ring topology -- CPU usage}
\label{fig:ring-cpu_usage}
\end{figure}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/ring/mem_usage.png}
\caption{Ring topology -- memory usage}
\label{fig:ring-mem_usage}
\end{figure}
\end{minipage}

\input{tex/figures/ring/avg}
\input{tex/figures/ring/total}


\newpage
\subsection{Case 3: Grid Topology}

\begin{wrapfigure}{r}{0.5\textwidth}
\centering
\captionsetup{justification=centering,width=0.5\linewidth}
\includegraphics[width=0.7\linewidth]{figures/topologies/grid.png}
\caption{Grid -- topology}
\label{fig:grid-topology}
\end{wrapfigure}

Grid topology shows a similar overall behaviour to ring topology, except one
big difference in terms of disk usage. In case of ring topology there are
several very distinctive spikes in both reading and writing, whereas in grid
topology the spikes are much smaller and the usage is smoother generally. We
think that is thanks to the fact that nodes have more neighbours and so if a
file becomes available at one node, then more other nodes can get access to it.
We also observe that the nodes with the least amount of activity are usually
the ones with the least amount of neighbours, namely: the nodes located in the
corners of the grid.

Figure \ref{fig:grid-topology} displays how nodes are connected to each other.
Exact numbers for every metric are displayed in tables \ref{tab:grid-avg} and
\ref{tab:grid-total}. Table \ref{tab:grid-avg} contains metrics averages for
every node. Similarly, table \ref{tab:grid-total} contains metrics totals for
every node.

\textcolor{white}{.}\\

\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/grid/blk_read.png}
\caption{Grid topology -- amount of bytes read from disk}
\label{fig:grid-net_read}
\end{figure}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/grid/blk_write.png}
\caption{Grid topology -- amount of bytes written to disk}
\label{fig:grid-net_write}
\end{figure}
\end{minipage}

\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/grid/cpu_usage.png}
\caption{Grid topology -- CPU usage}
\label{fig:grid-blk_read}
\end{figure}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/grid/mem_usage.png}
\caption{Grid topology -- memory usage}
\label{fig:grid-blk_write}
\end{figure}
\end{minipage}

\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/grid/net_read.png}
\caption{Grid topology -- amount of bytes received over network}
\label{fig:grid-cpu_usage}
\end{figure}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/grid/net_write.png}
\caption{Grid topology -- amount of bytes sent over network}
\label{fig:grid-mem_usage}
\end{figure}
\end{minipage}

\input{tex/figures/grid/avg}
\input{tex/figures/grid/total}


\newpage
\subsection{Case 4: Random Graph Topology}

\begin{wrapfigure}{r}{0.5\textwidth}
\centering
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=0.7\linewidth]{figures/topologies/graph-random.png}
\caption{Random graph -- topology}
\label{fig:graph-random-topology}
\end{wrapfigure}

Results for random graph topology are similar in a way to the ones for grid
topology. The same uneven load is observed in case of disk I/O as well as
network I/O, but in contrast to a grid, in a random graph there are little to
no spikes being observed. The big difference is CPU usage, which is not bursty
and more or less consistent across nodes, but it's spiky. Also, we observe that
the nodes that get the most network traffic are usually the ones that have many
neighbours, for example: nodes \verb|06|, \verb|01|, \verb|02|.

Figure \ref{fig:graph-random-topology} displays how nodes are connected to each
other. Exact numbers for every metric are displayed in tables
\ref{tab:graph-random-avg} and \ref{tab:graph-random-total}. Table
\ref{tab:graph-random-avg} contains metrics averages for every node. Similarly,
table \ref{tab:graph-random-total} contains metrics totals for every node.

\textcolor{white}{.}\\

\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/graph-random/blk_read.png}
\caption{Random graph topology -- amount of bytes read from disk}
\label{fig:graph-random-net_read}
\end{figure}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/graph-random/blk_write.png}
\caption{Random graph topology -- amount of bytes written to disk}
\label{fig:graph-random-net_write}
\end{figure}
\end{minipage}

\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/graph-random/cpu_usage.png}
\caption{Random graph topology -- CPU usage}
\label{fig:graph-random-blk_read}
\end{figure}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/graph-random/mem_usage.png}
\caption{Random graph topology -- memory usage}
\label{fig:graph-random-blk_write}
\end{figure}
\end{minipage}

\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/graph-random/net_read.png}
\caption{Random graph topology -- amount of bytes received over network}
\label{fig:graph-random-cpu_usage}
\end{figure}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/graph-random/net_write.png}
\caption{Random graph topology -- amount of bytes sent over network}
\label{fig:graph-random-mem_usage}
\end{figure}
\end{minipage}

\input{tex/figures/graph-random/avg}
\input{tex/figures/graph-random/total}


\newpage
\subsection{Case 5: Complete Graph Topology}

\begin{wrapfigure}{r}{0.5\textwidth}
\centering
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=0.7\linewidth]{figures/topologies/graph-complete.png}
\caption{Complete graph -- topology}
\label{fig:graph-complete-topology}
\end{wrapfigure}

Disk and network I/O is spread more or less evenly across nodes and are very
smooth, almost straight lines. CPU usage is the best among all topologies.
There are no nodes that are loaded more than others and they all and keep the
CPU usage very consistent start to finish at around 45\%. This is all thanks to
the fact that all nodes have all other nodes as neighbours, so they can
directly access the files on other instances without having to wait.

Figure \ref{fig:graph-complete-topology} displays how nodes are connected to
each other. Exact numbers for every metric are displayed in tables
\ref{tab:graph-complete-avg} and \ref{tab:graph-complete-total}. Table
\ref{tab:graph-complete-avg} contains metrics averages for every node.
Similarly, table \ref{tab:graph-complete-total} contains metrics totals for
every node.

\textcolor{white}{.}\\\\\\

\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/graph-complete/blk_read.png}
\caption{Complete graph topology -- amount of bytes read from disk}
\label{fig:graph-complete-net_read}
\end{figure}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/graph-complete/blk_write.png}
\caption{Complete graph topology -- amount of bytes written to disk}
\label{fig:graph-complete-net_write}
\end{figure}
\end{minipage}

\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/graph-complete/cpu_usage.png}
\caption{Complete graph topology -- CPU usage}
\label{fig:graph-complete-blk_read}
\end{figure}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/graph-complete/mem_usage.png}
\caption{Complete graph topology -- memory usage}
\label{fig:graph-complete-blk_write}
\end{figure}
\end{minipage}

\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/graph-complete/net_read.png}
\caption{Complete graph topology -- amount of bytes received over network}
\label{fig:graph-complete-cpu_usage}
\end{figure}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{figure}[H]
\captionsetup{justification=centering,width=0.8\linewidth}
\includegraphics[width=\linewidth]{figures/graph-complete/net_write.png}
\caption{Complete graph topology -- amount of bytes sent over network}
\label{fig:graph-complete-mem_usage}
\end{figure}
\end{minipage}

\input{tex/figures/graph-complete/avg}
\input{tex/figures/graph-complete/total}


\subsection{\textcolor{red}{Conclusion}}
...
best performance, worst performance, decent enough


\end{document}
